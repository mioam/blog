---
title: Deep Learning 学习笔记 02 - Supervised Learning (1)
date: 2021-03-08 19:08:44
tags: []
categories: 学习笔记
---


### 什么是监督学习(Supervised Learning)？

这一课讲的是监督学习，首先还是先介绍一下什么是监督学习。监督学习是机器学习的一种方法，或者是一种模式，用于与无监督学习区分。监督学习的框架是通过带标记的(labeled)数据集学习。带标记的数据集具体地来讲是一组对应的 $\{x^{(i)},y^{(i)}\}_i$，$x^{(i)}$ 是一个输入，$y^{(i)}$ 是对应的输出。一般来讲，监督学习是通过SGD用一个网络来拟合一个数据集，使得给出 $x^{(i)}$，网络的输出 $f(x^{(i)})$ 和 $y^{(i)}$ 比较接近（loss小）。

但是需要注意的是，我们训练网络的目的并非让它在训练集上的loss最小。这块大概有这样的逻辑：
1. 我有一个总分布(population distribution)$\mathcal D$，这是一个在自然界中真实存在的分布。我可以从中随机取出 $(x,y)\sim \mathcal D$。我希望能够通过 $x$ 预测出 $y$。也就是说，我需要拟合 $\mathcal D$。
2. 但是由于我并不能直接获得 $\mathcal D$，所以我只能从随机采样出一个数据集  $\mathcal S = \{x^{(i)},y^{(i)}\}_{i=1}^N$, $(x^{(i)},y^{(i)})\overset{i.i.d.}{\sim}\mathcal D$。我定义实验分布(empirical distribution)，$Pr[(x^{(i)},y^{(i)})]=1/N$。 我至少希望我的模型很好的拟合了实验分布。
3. 为了明确我的目的，我们引入损失函数，希望我的模型最小化一个分布上的损失函数
4. 但是拟合了实验分布并不代表拟合了总分布，比如 $f(x)=\left\{\begin{aligned}y^{(i)} &,\exists x^{(i)}\in \mathcal S, x=x^{(i)}\\ anything &, otherwise  \end{aligned}\right.$。为此，我把数据集分成两部分，训练集(training set)和验证集(validation set)，如果我的模型从训练集上学到的东西在验证集上拟合得很好，那么我可以认为他在总分布上也拟合的很好。
5. *请避免利用验证集进行大量微调(fine-tune)，如果需要，请再分出一个验证集（*

有几个重点：目的是总分布，用损失函数作为目标，只能在训练集上训。，

### 分类问题(Classification)

监督学习里有主要有两种类型：分类问题和回归问题(Regularization)。分类问题就是给你一些类型，你的网络能够对输入分类。一般来说你的网络输出是一个概率分布。

对于分类问题，我们一般希望找到
$$
\theta^* = \arg\min_{\theta\in \Theta} Loss_{\mathcal S}(\theta) = \arg\min_{\theta\in \Theta} \sum_{(x,y)\in \mathcal S} err(f(x;\theta),y)Pr_{\mathcal S}[(x,y)].
$$

这里 $err$ 是一个判断模型输出与 $y$ 的符合程度的东西，对于二分类问题（$y\in \{0,1\}$），可以定义 $err(p,y) = -\log p_y$，其中 $p_0+p_1=1$。（$p_0+p_1=1$这个限制可以让单个标量输出先过一个sigmoid得到 $p_0\in [0,1]$，然后 $p_1:=1-p_0$ 来得到。）

对于多分类问题，也可以用类似的方法，使用softmax函数，$softmax(z)_i = \frac{e^{z_i}}{\sum e^{z_i}}$。$err(p,y) = -\log p_y$。

这样算出来的东西正好是交叉熵(cross-entropy)。这个损失函数被成为NLL(negative log-likelihood) loss或cross-entropy loss。Softmax函数在分类问题中被广泛使用，一般来说，效果比L2-norm或者L1-norm好？。

//省略一些计算梯度的细节

#### 越深越好？

考虑计算梯度的时候每一层都会乘上一个 $\sigma'(x)$，但是注意到sigmoid和tanh在 $|x|$ 较大的时候，导数几乎为0，容易发生梯度消失。（具体可能之后讲？）

处理这个问题可以考虑ReLU。

#### ReLU

ReLU也有一些问题，比如ReLU在 $x=0$ 导数无定义。一个解决方法是使用次梯度(subgradients)，用任意一个0 ~ 1之间的数作为ReLU在 $x=0$ 的导数。但是实际上由于x是实数，这种情况几乎不会发生。类似的，我们可以处理maxpooling的梯度。

ReLU另一个问题是当 $x<0$，导数全为0。如果对数据集中所有数据，某个神经元的输出都是负数，经过ReLU就都是0，导数反向传递回来也都是0。这时候这个神经元的参数很难再变化了，这就是“神经元死亡”。有时候学习率太大，也很可能引发这样的现象。

一个好处是，这自动压缩了网络的大小，我们可以在训练结束后手动删除死亡的神经元。但是这个问题还是存在。对此人们想出了Leaky ReLU、ExponentialLU等解决方案，大概是把ReLU负数的那段稍微做一点变化。

//省略 http://playground.tensorflow.org/

### 卷积(convolution)

神经网络可以理解成对某种特征的提取。在用神经网络处理音频序列或者图像时，我们认为同样的一小段音频或者图案，无论在什么位置，应当都是一样的特征，于是我们想要把网络的一部分应用到图像的每个位置上。

具体来讲，我们希望某一层的一些神经元共享同样的参数但是作用的位置不同。用一维的输入，或者说一段音频为例，输入是 $x_1,...,x_n$。
$$
y_i = \sum_{j=0}^k x_{i+j} w_{j}
$$
其中 $w$ 是共享的权值，也被成为核(kernel)。这个操作被称为卷积。（实际上这个是corelation，但是机器学习上好像都叫卷积了。其实就是 $w$ 下标顺序反一下。）

利用卷积，我们可以将神经网络的某些层变成卷积层。用不同的卷积核，我们可以得到很多通道(channel)（类似图像的RGB三个通道），同时提取不同的特征。

卷积减小了参数的数量，也加速了计算。同时卷积通过加入先验知识“特征与位置无关”，限制了神经网络的能力，一定程度上减小了过拟合(overfit)。

//省略手动算卷积的例子

#### 填补边界?(Padding) 和 步幅?(stride)

注意之前讲的卷积，我们发现一个长度为 $n$ 的向量x，与一个长度为 $k$ 的卷积核w，卷积之后的长度变成了$n-k+1$。我们希望卷积之后的向量长度不变，那么我们就需要padding，也就是在最初的向量x两端加一些位置。这些位置的值有不同的设置方式，比如填常数，比如重复边界的值，比如镜像边界的值等。

除了padding，卷积的时候还可以调整stride，注意到直接计算，x的同一个位置会被统计很多次，我们可以定义一个stride，$s$。
$$
y_i = \sum_{j=0}^k x_{i\times s+j} w_{j}
$$

这样以=一来，我们成倍减小了向量的长度，也减小了计算的时间空间。

padding和stride也可以应用在二维的图像上。

//省略一些故事



